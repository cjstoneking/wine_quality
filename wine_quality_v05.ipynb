{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Attaching package: ‘GGally’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    nasa\n",
      "\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(GGally)\n",
    "library(reshape2)\n",
    "library(gridExtra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains R code for performing classification with ensembles of polynomial regression models. The procedure for generating and running an ensemble is as follows:\n",
    "\n",
    "Split data into cross-validation folds (e.g. 5 folds)\n",
    "    Set one fold aside as ensemble hold-out, rest are ensemble development data\n",
    "    Initialize ensemble as empty list\n",
    "    For each model in ensemble:\n",
    "        Initialize model (e.g. all variables present at power 1, all interactions present)\n",
    "        For each step of model improvement:\n",
    "            If the model has not been evaluated before: leave it unchanged, so the next steps evaluate baseline                       performance  \n",
    "            Else: Make a random change to the model (change the poynomial degree of a variable, or add/drop an                       interaction)\n",
    "            Split ensemble development data into cross-validation folds (e.g. 10 folds), \n",
    "                set one fold aside as model hold-out, rest are training data\n",
    "            Fit model to training data, evaluate on model hold-out data\n",
    "            If performance on model hold-out data is better than previous version of model, keep the change\n",
    "       Add model to ensemble\n",
    "   Fit each model in ensemble to entire development data, evaluate on ensemble hold-out data\n",
    "   Ensemble output is mean output of all models on ensemble hold-out\n",
    "            \n",
    "Note that we develop a different ensemble for each fold of the cross-validation folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      "[[1]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[1]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      "\n",
      "\n",
      "[[2]]\n",
      "[[2]][[1]]\n",
      " [1] 1 1 2 1 1 1 1 2 1 1 1 1\n",
      "\n",
      "[[2]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      "\n",
      "\n",
      "[[3]]\n",
      "[[3]][[1]]\n",
      " [1] 1 1 1 2 1 1 1 1 2 1 1 1\n",
      "\n",
      "[[3]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "\n",
      "\n",
      "[[4]]\n",
      "[[4]][[1]]\n",
      " [1] 1 1 0 1 1 1 1 2 1 1 1 0\n",
      "\n",
      "[[4]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[5]]\n",
      "[[5]][[1]]\n",
      " [1] 1 1 0 1 1 1 1 1 1 1 2 0\n",
      "\n",
      "[[5]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      "[112] 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.5575031 0.5533516 0.5526155 0.5528024 0.5576567\n",
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      "[[1]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 1 1 2 1 0\n",
      "\n",
      "[[1]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[2]]\n",
      "[[2]][[1]]\n",
      " [1] 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[2]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[3]]\n",
      "[[3]][[1]]\n",
      " [1] 1 1 0 1 1 1 1 2 1 1 1 0\n",
      "\n",
      "[[3]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "\n",
      "\n",
      "[[4]]\n",
      "[[4]][[1]]\n",
      " [1] 2 1 0 1 1 1 1 1 1 1 1 0\n",
      "\n",
      "[[4]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[5]]\n",
      "[[5]][[1]]\n",
      " [1] 1 1 2 1 1 1 1 1 1 1 1 0\n",
      "\n",
      "[[5]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
      "\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.5546623 0.5522179 0.5504359 0.5552369 0.5542352\n",
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      "[[1]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 2 1 1 1 1\n",
      "\n",
      "[[1]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "[112] 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[2]]\n",
      "[[2]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "\n",
      "[[2]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1\n",
      "\n",
      "\n",
      "[[3]]\n",
      "[[3]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 2 1 1 1 0\n",
      "\n",
      "[[3]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "[112] 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[4]]\n",
      "[[4]][[1]]\n",
      " [1] 1 3 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[4]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[5]]\n",
      "[[5]][[1]]\n",
      " [1] 1 1 1 2 0 1 1 1 1 1 1 0\n",
      "\n",
      "[[5]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.5490833 0.5510982 0.5478297 0.5526965 0.5487752\n",
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      "[[1]][[1]]\n",
      " [1] 1 2 1 2 1 2 1 1 1 1 1 1\n",
      "\n",
      "[[1]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " [38] 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "[112] 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[2]]\n",
      "[[2]][[1]]\n",
      " [1] 1 1 1 1 1 2 1 2 1 1 1 0\n",
      "\n",
      "[[2]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[3]]\n",
      "[[3]][[1]]\n",
      " [1] 2 1 1 1 1 2 1 2 2 1 1 0\n",
      "\n",
      "[[3]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[4]]\n",
      "[[4]][[1]]\n",
      " [1] 1 2 1 2 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[4]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " [75] 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[5]]\n",
      "[[5]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 2 1 1 1 1\n",
      "\n",
      "[[5]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
      "\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.5478101 0.5482020 0.5474988 0.5464783 0.5497116\n",
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      "[[1]][[1]]\n",
      " [1] 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[1]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[2]]\n",
      "[[2]][[1]]\n",
      " [1] 2 2 1 2 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[2]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[3]]\n",
      "[[3]][[1]]\n",
      " [1] 1 1 2 1 1 1 1 2 1 1 1 1\n",
      "\n",
      "[[3]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "[[4]]\n",
      "[[4]][[1]]\n",
      " [1] 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "[[4]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "\n",
      "\n",
      "[[5]]\n",
      "[[5]][[1]]\n",
      " [1] 1 1 1 2 1 1 1 1 1 1 1 0\n",
      "\n",
      "[[5]][[2]]\n",
      "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " [75] 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[112] 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.5581615 0.5530783 0.5531927 0.5573360 0.5546106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"mean of scores_1 =  0.570215587431641\"\n",
      "[1] \"mean of scores_2 =  0.551746620240019\"\n",
      "[1] \"SEM of scores_1 =  0.00398479051911142\"\n",
      "[1] \"SEM of scores_2 =  0.00448594318556725\"\n"
     ]
    }
   ],
   "source": [
    "#script for defining settings/hyperparameters and running ensemble classification\n",
    "hp <- get_default_hyperparameters()\n",
    "\n",
    "attr(hp, \"verbose\") <- TRUE\n",
    "\n",
    "attr(hp, \"run_baseline_model\") <- TRUE\n",
    "scores_1 <- evaluate_ensemble(hp)\n",
    "attr(hp, \"run_baseline_model\") <- FALSE\n",
    "scores_2 <- evaluate_ensemble(hp)\n",
    "\n",
    "print(paste(\"mean of scores_ = \", mean(scores_1), sep=\" \"))\n",
    "print(paste(\"mean of scores_2 = \", mean(scores_2), sep=\" \"))\n",
    "print(paste(\"SEM of scores_1 = \", sd(scores_1)/sqrt(length(scores_1)), sep=\" \"))\n",
    "print(paste(\"SEM of scores_2 = \", sd(scores_2)/sqrt(length(scores_2)), sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the default hyperparameter object\n",
    "get_default_hyperparameters <- function(){\n",
    "    \n",
    "    hyperparameters <- list()\n",
    "    \n",
    "    #dataset-specific settings:\n",
    "    \n",
    "    attr(hyperparameters, \"main_url\") <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/\"\n",
    "    attr(hyperparameters, \"files\")    <- c(\"winequality-red.csv\", \"winequality-white.csv\")\n",
    "    attr(hyperparameters, \"response_name\") <- \"quality\"\n",
    "    \n",
    "    \n",
    "    #hyperparameters of the ensemble:\n",
    "    \n",
    "    attr(hyperparameters, \"initial_model_test\") <- FALSE\n",
    "    #if this is set to TRUE, then do not attempt to improve linear models or form ensemble\n",
    "    #instead just fit a single linear model to data, with powers given by initial settings below\n",
    "    \n",
    "    attr(hyperparameters, \"initial_power\") <- 1\n",
    "    attr(hyperparameters, \"initial_interaction\") <- 1\n",
    "    #polynomial powers for variables and interactions in the initial linear model\n",
    "    #both set to 1: initial model has all variables and all interactions\n",
    "    #both set to 0: initial model is empty (has only intercept)\n",
    "    #variables = 1, interactions = 0 : initial model has all variables, no interactions\n",
    "    #(these are probably the only sensible settings)\n",
    "    \n",
    "    attr(hyperparameters, \"top_level_repeats\") <- 1\n",
    "    #number of repeats to perform at top level (evaluating entire ensemble-forming strategy)\n",
    "    \n",
    "    attr(hyperparameters, \"ensemble_K\")  <- 5\n",
    "    #number of folds of cross-validation for evaluating entire ensembles\n",
    "    \n",
    "    \n",
    "    attr(hyperparameters, \"ensemble_size\") <- 5\n",
    "    #number of models in ensemble\n",
    "    attr(hyperparameters, \"n_attempts\") <- 20\n",
    "    #number of attempts made to improve a single regression model\n",
    "    #(by changing the polynomial degreee of variables, or adding/dropping interactions)\n",
    "    \n",
    "    attr(hyperparameters, \"single_model_K\") <- 5\n",
    "    #number of folds of cross-validation for evaluating single models\n",
    "    \n",
    "    attr(hyperparameters, \"max_power\") <- 4\n",
    "    #max polynomial power to raise a variable to \n",
    "    attr(hyperparameters, \"weight_by_scores\") <- FALSE\n",
    "    #whether to weight the single models in the ensemble by their performance on out-of-fold data\n",
    "    #when taking the average over all models\n",
    "    \n",
    "    attr(hyperparameters, \"P_change_interactions\") <- 0.5\n",
    "    #probability of changing an interaction in a step of model adjustment\n",
    "    #as opposed to changing a variable power\n",
    "    return(hyperparameters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function for evaluating different approaches\n",
    "# hyperparameters : list with settings and parameters for ensemble generation\n",
    "# returns: vector of scores of ensembles on hold-out data\n",
    "evaluate_ensemble <- function(hyperparameters){\n",
    "\n",
    "    verbose <- attr(hyperparameters, \"verbose\")\n",
    "    K <- attr(hyperparameters, \"ensemble_K\")\n",
    "    R <- attr(hyperparameters, \"top_level_repeats\")\n",
    "    \n",
    "    main_url <- attr(hyperparameters, \"main_url\")\n",
    "    files    <- attr(hyperparameters, \"files\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    have_data <- find_or_download_data(main_url, files)\n",
    "    if(!have_data){\n",
    "        print(\"Could not find or download data - returning null\")\n",
    "        return(NULL)\n",
    "    }\n",
    "    else{\n",
    "        \n",
    "        data <- read_data(files)\n",
    "        \n",
    "        data <- preprocess(data, response_name)\n",
    "        \n",
    "        CV_indices <- get_CV_folds(nrow(data), K)\n",
    "        train_indices <- CV_indices[[1]]\n",
    "        test_indices  <- CV_indices[[2]]\n",
    "        \n",
    "        score_matrix = matrix(0, R, K)\n",
    "        \n",
    "        for (r in 1:R){\n",
    "            if(verbose){\n",
    "                #print(paste('Top-level repeat', r, sep=' '))\n",
    "            }\n",
    "            for (k in 1:K){\n",
    "                if(verbose){\n",
    "                    #print(paste('Top-level CV fold', k, sep=' '))\n",
    "                }\n",
    "                if(attr(hyperparameters, \"test_initial_model\")){\n",
    "                    #just fit one linear model to original predictors\n",
    "                    #for comparison\n",
    "                    #train_data <- data[train_indices[[k]],]\n",
    "                    #test_data  <- data[test_indices[[k]],]\n",
    "                    initial_power       <- attr(hyperparameters, \"initial_power\")\n",
    "                    initial_interaction <- attr(hyperparameters, \"initial_interaction\")\n",
    "                    \n",
    "                    #model  <- lm(y ~ ., data=train_data)\n",
    "                    #score_matrix[r,k]  <- mean(abs(test_data$y- predict(model, newdata=test_data)))\n",
    "                    npred <- ncol(data)-1\n",
    "                    powers <- rep(initial_power, npred)\n",
    "                    interactions <- rep(initial_interaction, npred*npred)\n",
    "                    \n",
    "                    predictions <- polyreg(data, powers, interactions, train_indices[[k]], test_indices[[k]])\n",
    "                    score_matrix[r,k] <- mean(abs(data[test_indices[[k]],]$y-predictions))\n",
    "                    \n",
    "                }\n",
    "                else{\n",
    "                    #first step: find an ensemble of models\n",
    "                    #by searching the space of possible models\n",
    "                    #(this is the space of 2^P possible subsets of all predictors)\n",
    "                    outputs <- find_ensemble(data[train_indices[[k]],])\n",
    "                    scores <- outputs[[1]]\n",
    "                    ensemble_definitions <- outputs[[2]]\n",
    "                    if(verbose){\n",
    "                        print('Model definitions:')\n",
    "                        print(ensemble_definitions)\n",
    "                        print('Scores:')\n",
    "                        print(scores)\n",
    "                    }\n",
    "                    #now train the ensemble on the same data\n",
    "                    predictions <- run_ensemble(ensemble_definitions, data, train_indices[[k]], test_indices[[k]], scores, hyperparameters)\n",
    "                    score_matrix[r,k] <- mean(abs(predictions - data$y[test_indices[[k]]]))\n",
    "                    #print(paste(\"Ensemble score =\",score,sep=\" \"))\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    dim(score_matrix) <- NULL\n",
    "    return(score_matrix)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score predictors\n",
    "preprocess <- function(data, response_name, hyperparameters=get_default_hyperparameters()){\n",
    "    \n",
    "    #separate into response and predictors\n",
    "    response <- data[[response_name]]\n",
    "    predictors <- data\n",
    "    predictors[[response_name]] <- NULL\n",
    "        \n",
    "    #z-score predictors\n",
    "    znorm <- function(x)  (x - mean(x))/ sd(x)\n",
    "    predictors <- apply(predictors, 2, znorm)\n",
    "    predictors <- as.data.frame(predictors)\n",
    "        \n",
    "    #combine into single matrix again\n",
    "    data <- cbind(response, predictors)\n",
    "    colnames(data)[1] <- \"y\"\n",
    "    \n",
    "    return(data)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a trained ensemble on hold-out data\n",
    "run_ensemble <- function(ensemble_models, data, train_indices, test_indices, scores=NULL, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "    \n",
    "    weight_by_scores <- attr(hyperparameters, \"weight_by_scores\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    ensemble_size <- length(ensemble_models)\n",
    "    predictions <- vector(\"list\", ensemble_size)\n",
    "    for (n in 1:ensemble_size){\n",
    "        if(verbose){\n",
    "            print(paste(\"running model\", n, \"/\", ensemble_size, sep=\" \"))\n",
    "        }\n",
    "        \n",
    "        models <- ensemble_models[[n]]\n",
    "        \n",
    "        powers <- models[[1]]\n",
    "        interactions <- models[[2]]\n",
    "        \n",
    "        predictions[[n]] <- polyreg(data, powers, interactions, train_indices, test_indices)\n",
    "    }\n",
    "    predictions_matrix <- t(Reduce(rbind, predictions))\n",
    "    \n",
    "    weights = rep(1/length(ensemble_models), length(ensemble_models))\n",
    "    if(length(scores)>0 && weight_by_scores){\n",
    "        weights = -log(scores)\n",
    "        weights = weights/sum(weights)\n",
    "    }\n",
    "    else{\n",
    "        #assume NULL was passed for scores\n",
    "        weights = rep(1/length(ensemble_models), length(ensemble_models))\n",
    "    }\n",
    "    weighted_avg <- rep(0, length(test_indices))\n",
    "    for (n in 1:ensemble_size){    \n",
    "        weighted_avg <- weighted_avg + weights[n]*predictions_matrix[,n]\n",
    "    }\n",
    "    \n",
    "    return(weighted_avg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ensemble by generating multiple single models\n",
    "find_ensemble <- function(data, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "     \n",
    "    ensemble_size <- attr(hyperparameters, \"ensemble_size\")\n",
    "    ensemble = vector(\"list\", ensemble_size)\n",
    "    scores = rep(0, ensemble_size)\n",
    "    \n",
    "    for (n in 1:ensemble_size){\n",
    "    \n",
    "        if(verbose){\n",
    "            print(paste(\"finding model\", n, \"/\", ensemble_size, sep=\" \"))\n",
    "        }\n",
    "        outputs <- find_single_model(data, hyperparameters=hyperparameters, verbose=verbose)\n",
    "        scores[n] <- outputs[[1]]\n",
    "        ensemble[[n]] <- outputs[2:3]\n",
    "    }\n",
    "    return(list(scores, ensemble))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for polynomial-regression-based prediction\n",
    "# data   : data matrix\n",
    "# powers : vector of polynomial powers that each variable is raised to\n",
    "#          power = 0 : left out of model\n",
    "# interactions : vector of interactions\n",
    "#                if there are P predictors, i > j  and interactions[(i-1)*P + j] > 0, \n",
    "#                then the interaction between predictors i and j is in the model\n",
    "#                (irrespective of the polynomial powers of predictors i and j)\n",
    "#train_indices : indices of rows to use for training model\n",
    "#test_indices  : indices of rows to use for testing/prediction\n",
    "polyreg <- function(data, powers, interactions, train_indices, test_indices){\n",
    "    \n",
    "    response <- data$y\n",
    "    predictors <- data\n",
    "    predictors[[\"y\"]] <- NULL\n",
    "    pred_poly_matrix <- NULL\n",
    "    for(p in 1:ncol(predictors)){\n",
    "        if(powers[[p]] > 0){\n",
    "            m <- poly(predictors[[p]], powers[[p]])\n",
    "            colnames(m) <- paste(colnames(predictors)[p], colnames(m), sep=\"\")\n",
    "            if(is.null(pred_poly_matrix)){\n",
    "                pred_poly_matrix <- m\n",
    "            }\n",
    "            else{\n",
    "                pred_poly_matrix <- cbind(pred_poly_matrix, m)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    for(p1 in 1:ncol(predictors)){\n",
    "        for(p2 in 1:ncol(predictors)){\n",
    "\n",
    "            if(p1>p2 && interactions[(p1-1)*ncol(predictors) + p2] > 0){\n",
    "                pred_poly_matrix <- cbind(predictors[[p1]]*predictors[[p2]], pred_poly_matrix)\n",
    "                colnames(pred_poly_matrix)[1] <- paste(colnames(predictors)[p1], colnames(predictors)[p2], sep=\"x\")\n",
    "            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    full_data <- as.data.frame(cbind(response, pred_poly_matrix))\n",
    "    colnames(full_data)[1] <- \"y\"\n",
    "    \n",
    "    #now the full_data dataframe includes all the powers and interactions that we want\n",
    "    #so the formula is just y ~. , i.e. regress y onto just the columns of the dataframe as they are, \n",
    "    #do not add any more powers/interactions    \n",
    "    model  <- lm(y ~ ., data=full_data[train_indices,])\n",
    "    \n",
    "    options(warn=-1)      #turn off warnings\n",
    "    prediction <- predict(model, newdata=full_data[test_indices,])\n",
    "    options(warn=1) \n",
    "            \n",
    "    return(prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a single model\n",
    "#to serve as part of an ensemble\n",
    "#by starting with initial polynomial regression model \n",
    "#and iteratively attempting to improve it\n",
    "find_single_model <- function(data, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "    \n",
    "    K <- attr(hyperparameters, \"single_model_K\")\n",
    "    CV_folds   <- get_CV_folds(nrow(data), K)\n",
    "    train_indices <- CV_folds[[1]]\n",
    "    test_indices <- CV_folds[[2]]\n",
    "    \n",
    "    n_attempts <- attr(hyperparameters, \"n_attempts\")\n",
    "    max_power  <- attr(hyperparameters, \"max_power\")\n",
    "    starting_power <- attr(hyperparameters, \"starting_power\")\n",
    "    do_interaction <- attr(hyperparameters, \"P_change_interactions\")\n",
    "    \n",
    "    response <- data$y\n",
    "    predictors <- data\n",
    "    predictors[[\"y\"]] <- NULL\n",
    "    n_predictors <- ncol(predictors)\n",
    "    \n",
    "    current_powers <- rep(starting_power, n_predictors)\n",
    "    best_powers <- rep(starting_power, n_predictors)\n",
    "    current_interactions <- rep(starting_power, n_predictors*n_predictors)\n",
    "    best_interactions <- rep(starting_power, n_predictors*n_predictors)\n",
    "    is_dummy <- rep(FALSE, n_predictors)\n",
    "    for(i in 1:length(is_dummy)){\n",
    "        is_dummy[i] <- length(unique(predictors[[i]])) <= 2\n",
    "    }\n",
    "    \n",
    "    best_score <- Inf\n",
    "    \n",
    "    null_model_score <- 0\n",
    "    \n",
    "    for(n in 1:n_attempts){\n",
    "        \n",
    "        \n",
    "        if(n == 1){\n",
    "            #do nothing here in the first iteration\n",
    "        }\n",
    "        else{\n",
    "            #in iterations after the first, make random changes to the model\n",
    "            \n",
    "            \n",
    "            if(runif(1) < do_interaction){\n",
    "                #adjust one randomly chosen interaction\n",
    "                ps <- sample(1:n_predictors, 2, replace=FALSE)\n",
    "                p1 <- max(ps)\n",
    "                p2 <- min(ps)\n",
    "                index <- (p1-1)*n_predictors + p2\n",
    "                if(current_interactions[index]==1){\n",
    "                    current_interactions[index]<-0\n",
    "                }\n",
    "                else{\n",
    "                    current_interactions[index]<-1  \n",
    "                }\n",
    "            }\n",
    "            else{\n",
    "                #adjust power of a randomly chosen variable\n",
    "                index <- sample(1:n_predictors, 1)\n",
    "                sign  <- sample(c(-1, 1), 1)\n",
    "                if(current_powers[index] == max_power && sign==1){\n",
    "                    sign <- -1\n",
    "                }\n",
    "                if(current_powers[index]==0 && sign==-1){\n",
    "                    sign <-  1\n",
    "                }\n",
    "                if(is_dummy[index] && current_powers[index] == 1){\n",
    "                    sign <- -1\n",
    "                }\n",
    "                current_powers[index] <- current_powers[index] + sign\n",
    "            }\n",
    "        }\n",
    "        if(verbose){\n",
    "            print(paste(\"Evaluating model:\", paste(current_powers, sep=\" \", collapse=\", \"), sep=\" \"))\n",
    "        }\n",
    "        \n",
    "        oof_scores <- rep(0, K)\n",
    "        \n",
    "        for(k in 1:K){\n",
    "            if(sum(current_powers)==0){\n",
    "                #empty model - just use mean as intercept\n",
    "                train_Y <- response[train_indices[[k]]]\n",
    "                test_Y  <- response[test_indices[[k]]] \n",
    "                prediction <- mean(train_Y)\n",
    "                oof_scores[k] <- mean(abs(test_Y - prediction))\n",
    "            }\n",
    "            else{\n",
    "                \n",
    "                y_pred <- polyreg(data, current_powers, current_interactions, train_indices[[k]], test_indices[[k]])\n",
    "            \n",
    "                oof_scores[k] <- mean(abs(response[test_indices[[k]]] - y_pred))\n",
    "            }\n",
    "\n",
    "        }#end loop over folds\n",
    "        \n",
    "        if(n==1){\n",
    "            starting_model_score <- mean(oof_scores)\n",
    "        }\n",
    "        \n",
    "        #if the model is an improvement over previous best, update it\n",
    "        final_score <- mean(oof_scores)\n",
    "        \n",
    "        if(final_score < best_score){\n",
    "            output_message = \"Keeping model\"\n",
    "            best_score <- final_score\n",
    "            best_powers <- current_powers\n",
    "            best_interactions <- current_interactions\n",
    "        }\n",
    "        else{\n",
    "            output_message = \"Discarding model\"\n",
    "            current_powers <- best_powers\n",
    "            current_interactions <- best_interactions\n",
    "        }\n",
    "        if(verbose){\n",
    "            print(paste(\"Model score =\", final_score, sep=\" \"))\n",
    "            print(output_message)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #return description of this model\n",
    "    #as a list with: CV score, vector of powers, vector of interactions\n",
    "    return(list(best_score, best_powers, best_interactions))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if data files are present in local directory\n",
    "#if they are not, try to download\n",
    "#return 1 if files are now present locally (either found or successfully downloaded)\n",
    "#return 0 otherwise\n",
    "#if this returns 1, we expect a subsequent call to read_data to succeed\n",
    "find_or_download_data <- function (main_url, files){\n",
    "\n",
    "    for (i in 1:length(files)){\n",
    "        if(!file.exists(files[i])){\n",
    "            print(paste(\"Could not find\", files[i], \"- attempting to download\", sep=\" \" ))\n",
    "            download.file(paste(main_url, files[i],sep=\"\") , files[i], \"auto\", quiet = FALSE)\n",
    "        }\n",
    "    }\n",
    "    for (i in 1:length(files)){\n",
    "        if(!file.exists(files[i])){\n",
    "            print(paste(\"Could not find or download\", files[i], \"- training will fail\", sep=\" \" ))\n",
    "            return(0)\n",
    "        }\n",
    "    }\n",
    "    return(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from .csv files to data.frame\n",
    "#merge contents of multiple .csv into one dataframe (binds rows)\n",
    "#add a one-hot encoding of the file number\n",
    "read_data <- function(files){\n",
    "    single_datasets <- vector(\"list\", length(files))\n",
    "    for (i in 1:length(files)){\n",
    "        single_datasets[[i]] <- read.csv(files[i], header=TRUE, sep=\";\")\n",
    "        single_datasets[[i]]$file.number <- i\n",
    "    }\n",
    "    data <- bind_rows(single_datasets) \n",
    "    #bind_rows : dplyr function\n",
    "    \n",
    "    #data is now full dataset (merged across all files)\n",
    "        \n",
    "    #make one-hot encoding of file number\n",
    "    u <- unique(data$file.number)\n",
    "    for (val in u[1:(length(u)-1)]){\n",
    "        data[paste(\"file.number.\",val,sep=\"\")] <- (data$file.number==val)*1.0\n",
    "    }\n",
    "    data$file.number <- NULL\n",
    "    #remove initial (categorical) file number column\n",
    "    return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample indices of crossvalidation folds\n",
    "# N  : number of data points\n",
    "# K  : number of folds\n",
    "get_CV_folds <- function(N, K){\n",
    "    train_indices <- vector(\"list\", K)\n",
    "    test_indices  <- vector(\"list\", K)\n",
    "    random_permutation <- sample(N, N, replace=FALSE)\n",
    "    for (i in 1:K){\n",
    "        start <- floor(N*(i-1)/K) + 1\n",
    "        stop  <- floor(N*i/K)\n",
    "        test_indices[[i]] <-  random_permutation[start:stop]\n",
    "        train_indices[[i]] <- random_permutation[-(start:stop)]\n",
    "    }\n",
    "    return(list(train_indices, test_indices))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define shorter names for this specific dataset\n",
    "#to help plotting\n",
    "short_names <- function(){\n",
    "    sn <- list(fixed.acidity=\"fix.acd\",\n",
    "               volatile.acidity=\"vol.acd\",\n",
    "                citric.acid=\"cit.acd\",\n",
    "                residual.sugar=\"res.sug\",\n",
    "                chlorides=\"chlor\",\n",
    "                free.sulfur.dioxide=\"fr.SO2\",\n",
    "                total.sulfur.dioxide=\"tot.SO2\",\n",
    "                density=\"dens\",\n",
    "                pH=\"pH\",\n",
    "                sulphates=\"sulph\",\n",
    "                alcohol=\"alc\",\n",
    "                file.number.1=\"red\")\n",
    "    return(sn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for EDA\n",
    "library(gridExtra)\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "\n",
    "\n",
    "reorder_cormat <- function(cormat){\n",
    "    # Use correlation between variables as distance\n",
    "    dd <- as.dist((1-cormat)/2)\n",
    "    hc <- hclust(dd)\n",
    "    cormat <-cormat[hc$order, hc$order]\n",
    "}\n",
    "\n",
    "get_breaks <- function(x, margin, n_breaks){\n",
    "    m <- margin*(max(x) - min(x))\n",
    "    \n",
    "    return(seq(min(x) + m, max(x) - m, (max(x) - min(x) - 2*m)/(n_breaks-1)))\n",
    "}\n",
    "\n",
    "#main function for EDA\n",
    "run_eda <- function(to_show, hyperparameters, corr_cutoff=0.5, boxplot_predictor_groups=3){\n",
    "    main_url <- attr(hyperparameters, \"main_url\")\n",
    "    files    <- attr(hyperparameters, \"files\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    have_data <- find_or_download_data(main_url, files)\n",
    "    if(!have_data){\n",
    "        print(\"Could not find or download data - returning null\")\n",
    "        return(NULL)\n",
    "    }\n",
    "    else{\n",
    "        data <- read_data(files)\n",
    "        \n",
    "        #plot relationship between each predictor and response\n",
    "        predictor_names <- colnames(data)[colnames(data) != response_name]\n",
    "        \n",
    "        if(to_show==\"pairs\"){\n",
    "        #pairs plot\n",
    "            \n",
    "            sn <- short_names()\n",
    "            labels <- list()\n",
    "            for (i in 1:length(predictor_names)){\n",
    "                labels[i] <- sn[[predictor_names[i]]]\n",
    "            }\n",
    "        \n",
    "            pairs(data[,predictor_names], labels=labels, panel=points, pch = 16, cex = .5, xaxt = \"n\", yaxt = \"n\")\n",
    "            \n",
    "            #alternative: ggally pairs plot\n",
    "            #this does not handle the large number of variables well\n",
    "        \n",
    "            ## code for ggally pairs plot:\n",
    "            #pairplot_columns <- 1:length(colnames(data))\n",
    "            #pairplot_columns <- pairplot_columns[colnames(data) != response_name]\n",
    "            #pairplot <- ggpairs(data, columns=pairplot_columns) + \n",
    "            #  ggtitle(\"Pairs plot\")\n",
    "            #print(pairplot)\n",
    "        }\n",
    "        else if(to_show==\"corr_heatmap\"){\n",
    "        #correlation heatmap\n",
    "        \n",
    "            predictors <- data[,predictor_names]\n",
    "            colnames(predictors) <- labels\n",
    "        \n",
    "            cormat <- round(cor(predictors),2)\n",
    "            cormat <- reorder_cormat(cormat)\n",
    "        \n",
    "            cormat[lower.tri(cormat)]<- NA\n",
    "        \n",
    "            # Melt the correlation matrix\n",
    "            melted_cormat <- melt(cormat, na.rm = TRUE)\n",
    "            # Heatmap\n",
    "            ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+\n",
    "                 geom_tile(color = \"white\")+\n",
    "                 scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n",
    "                 midpoint = 0, limit = c(-1,1), space = \"Lab\", \n",
    "                 name=\"Pearson\\nCorrelation\") +\n",
    "                 theme_minimal()+ \n",
    "                 theme(axis.text.x = element_text(angle = 45, vjust = 1, \n",
    "                 size = 12, hjust = 1))+\n",
    "                 coord_fixed()\n",
    "        }\n",
    "        else if(to_show==\"corr_values\"){\n",
    "        #print pairs of variables with largest absolute correlation values\n",
    "\n",
    "            melted_cormat <- melt(cormat, na.rm=TRUE)\n",
    "            melted_cormat <- melted_cormat[order(abs(melted_cormat$value)),]\n",
    "            print(melted_cormat[abs(melted_cormat$value) > corr_cutoff & melted_cormat$Var1 != melted_cormat$Var2,])\n",
    "        }\n",
    "        else if(to_show==\"response_hist\"){\n",
    "        #plot histogram of response values\n",
    "            par(pin=c(4,3))\n",
    "            vals <- data[[response_name]]\n",
    "            h <- hist(vals, xlab=response_name, ylab=\"count\", breaks=seq(min(vals)-0.5, max(vals) + 0.5, 1), \n",
    "                 main=paste(\"Histogram of \", response_name, sep=\" \"),ylim=c(0, 3000))\n",
    "            #plot(h, )\n",
    "            scale <- length(vals)\n",
    "            curve(dnorm(x, mean=mean(vals), sd=sd(vals))*scale, add=TRUE, col=\"darkblue\", lwd=2)\n",
    "\n",
    "        }\n",
    "        else if(to_show==\"boxplots_grouped_by_response\"){\n",
    "        #plot series of boxplots grouped by values of response (on x axis)\n",
    "        #only works if response takes discrete values\n",
    "        #has some limitations, but can be useful\n",
    "            \n",
    "            label_format <- function(x) sprintf(\"%.2f\", x)\n",
    "            breaks <- function(x) get_breaks(x, 0.05, 4)\n",
    "            \n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    plots[[i]] <- ggplot(data, aes_string(group=response_name, y = x, x = response_name)) + \n",
    "                        geom_boxplot(outlier.size=0.15) + geom_smooth(aes(group=1), method=\"lm\") + \n",
    "                        scale_y_continuous(labels=label_format) + \n",
    "                        theme(axis.text.x = element_text(color = \"grey20\", size = 8, angle = 90, hjust = .5, vjust = .5, face = \"plain\")) +\n",
    "                        coord_flip()\n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        }\n",
    "        else if(to_show==\"boxplots_predictor_on_x\"){\n",
    "        #plot series of boxplots grouped by values of predictor (on x axis)  \n",
    "        #divide predictor into quantiles for grouping\n",
    "        #this tends to give ugly plots with issues that make them hard to interpret\n",
    "        #not used further\n",
    "            qstep <- 1/boxplot_predictor_groups\n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    quantile_boundaries <- quantile(data[[x]], probs=seq(0,1,qstep))\n",
    "                    quantile_groups <- rep(0, length(data[[x]]))\n",
    "                    for (q in quantile_boundaries){\n",
    "                        quantile_groups <- quantile_groups + 1.0*(data[[x]] > q)\n",
    "                    }\n",
    "                    plots[[i]] <- ggplot(data, aes_string(group=quantile_groups, y = response_name, x = x)) + \n",
    "                    geom_boxplot(outlier.size=0.15) + geom_smooth(aes(group=1), method=\"lm\") \n",
    "                        \n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        }\n",
    "        else if(to_show==\"separate_regressions\"){\n",
    "            \n",
    "            label_format <- function(x) sprintf(\"%.2f\", x)\n",
    "            \n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    plots[[i]] <- ggplot(data, aes_string(y = x, x = response_name)) + \n",
    "                        geom_count() + geom_smooth(aes(group=1), method=\"lm\") + theme(legend.position = \"none\") +\n",
    "                        scale_y_continuous(labels=label_format) + \n",
    "                        theme(axis.text.x = element_text(color = \"grey20\", size = 8, angle = 90, hjust = .5, vjust = .5, face = \"plain\")) +\n",
    "                        coord_flip()\n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        } \n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
