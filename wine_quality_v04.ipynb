{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(GGally)\n",
    "library(reshape2)\n",
    "library(gridExtra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, have one hyperparameters object that contains all variables\n",
    "get_default_hyperparameters <- function(){\n",
    "    \n",
    "    hyperparameters <- list()\n",
    "    \n",
    "    #dataset-specific settings:\n",
    "    attr(hyperparameters, \"main_url\") <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/\"\n",
    "    attr(hyperparameters, \"files\")    <- c(\"winequality-red.csv\", \"winequality-white.csv\")\n",
    "    attr(hyperparameters, \"response_name\") <- \"quality\"\n",
    "    \n",
    "    \n",
    "    attr(hyperparameters, \"ensemble_K\")  <- 5\n",
    "    #number of folds of cross-validation for evaluating entire ensembles\n",
    "    attr(hyperparameters, \"top_level_repeats\") <- 10\n",
    "    #number of repeats to perform at top level (evaluating entire ensemble-forming strategy)\n",
    "    attr(hyperparameters, \"ensemble_size\") <- 3\n",
    "    attr(hyperparameters, \"n_attempts\") <- 10\n",
    "    attr(hyperparameters, \"single_model_K\") <- 5\n",
    "    #number of folds of cross-validation for finding single models\n",
    "    attr(hyperparameters, \"starting_power\") <- 1\n",
    "    attr(hyperparameters, \"max_power\") <- 4\n",
    "    attr(hyperparameters, \"weight_by_scores\") <- TRUE\n",
    "    attr(hyperparameters, \"run_standard_model\") <- FALSE\n",
    "    attr(hyperparameters, \"new_features\") <- FALSE\n",
    "    return(hyperparameters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does all preprocessing (normalization etc.)\n",
    "#and defines new features by applying transformations\n",
    "preprocess <- function(data, response_name, hyperparameters=get_default_hyperparameters()){\n",
    "    \n",
    "    #separate into response and predictors\n",
    "    response <- data[[response_name]]\n",
    "    predictors <- data\n",
    "    predictors[[response_name]] <- NULL\n",
    "        \n",
    "    #z-score predictors\n",
    "    znorm <- function(x)  (x - mean(x))/ sd(x)\n",
    "    predictors <- apply(predictors, 2, znorm)\n",
    "    \n",
    "    predictors <- as.data.frame(predictors)\n",
    "    \n",
    "    #define new features\n",
    "    if(attr(hyperparameters, \"new_features\")){\n",
    "        original_features <- colnames(predictors)\n",
    "        for(i in 1:length(original_features)){\n",
    "            f <- original_features[[i]]\n",
    "            predictors[[paste(\"log.\",f,sep='')]] <- log(predictors[[f]] - min(predictors[[f]]) + 1)\n",
    "            predictors[[paste(\"sqrt.\",f,sep='')]] <- sqrt(predictors[[f]] - min(predictors[[f]]))\n",
    "            predictors[[paste(\"inv.\",f,sep='')]] <- 1/(predictors[[f]] - min(predictors[[f]] + 1))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    #combine into single matrix again\n",
    "    data <- cbind(response, predictors)\n",
    "    colnames(data)[1] <- \"y\"\n",
    "    \n",
    "    return(data)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ensemble <- function(ensemble_models, data, train_indices, test_indices, scores=NULL, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "    \n",
    "    weight_by_scores <- attr(hyperparameters, \"weight_by_scores\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    ensemble_size <- length(ensemble_models)\n",
    "    predictions <- vector(\"list\", ensemble_size)\n",
    "    for (n in 1:ensemble_size){\n",
    "        if(verbose){\n",
    "            print(paste(\"running model\", n, \"/\", ensemble_size, sep=\" \"))\n",
    "        }\n",
    "        \n",
    "        powers <- ensemble_models[[n]]\n",
    "        \n",
    "        predictions[[n]] <- polyreg(data, powers, train_indices, test_indices)\n",
    "    }\n",
    "    predictions_matrix <- t(Reduce(rbind, predictions))\n",
    "    \n",
    "    weights = rep(1/length(ensemble_models), length(ensemble_models))\n",
    "    if(length(scores)>0 && weight_by_scores){\n",
    "        weights = -log(scores)\n",
    "        weights = weights/sum(weights)\n",
    "    }\n",
    "    else{\n",
    "        #assume NULL was passed for scores\n",
    "        weights = rep(1/length(ensemble_models), length(ensemble_models))\n",
    "    }\n",
    "    weighted_avg <- rep(0, nrow(data))\n",
    "    for (n in 1:ensemble_size){    \n",
    "        weighted_avg <- weighted_avg + weights[n]*predictions_matrix[,n]\n",
    "    }\n",
    "    \n",
    "    return(weighted_avg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_ensemble <- function(data, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "    \n",
    "    \n",
    "    ensemble_size <- attr(hyperparameters, \"ensemble_size\")\n",
    "    ensemble = vector(\"list\", ensemble_size)\n",
    "    scores = rep(0, ensemble_size)\n",
    "    \n",
    "    for (n in 1:ensemble_size){\n",
    "    \n",
    "        if(verbose){\n",
    "            print(paste(\"finding model\", n, \"/\", ensemble_size, sep=\" \"))\n",
    "        }\n",
    "        outputs <- find_single_model(data, hyperparameters=hyperparameters, verbose=verbose)\n",
    "        scores[n] <- outputs[[1]]\n",
    "        ensemble[[n]] <- outputs[[2]]\n",
    "    }\n",
    "    return(list(scores, ensemble))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for polynomial-regression-based prediction\n",
    "polyreg <- function(data, powers, train_indices, test_indices){\n",
    "    \n",
    "    response <- data$y\n",
    "    predictors <- data\n",
    "    predictors[[\"y\"]] <- NULL\n",
    "    pred_poly_matrix <- NULL\n",
    "    for(p in 1:ncol(predictors)){\n",
    "        if(powers[[p]] > 0){\n",
    "            m <- poly(predictors[[p]], powers[[p]])\n",
    "            colnames(m) <- paste(colnames(predictors)[p], colnames(m), sep=\"\")\n",
    "            if(is.null(pred_poly_matrix)){\n",
    "                pred_poly_matrix <- m\n",
    "            }\n",
    "            else{\n",
    "                pred_poly_matrix <- cbind(pred_poly_matrix, m)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    full_data <- as.data.frame(cbind(response, pred_poly_matrix))\n",
    "    colnames(full_data)[1] <- \"y\"\n",
    "\n",
    "    print(\"until reg\")\n",
    "    \n",
    "    model  <- lm(y ~ (.)^2, data=full_data[train_indices,])\n",
    "            \n",
    "    return(predict(model, newdata=full_data[test_indices,]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a single model\n",
    "#to serve as part of an ensemble\n",
    "#return set of predictors used for that model\n",
    "find_single_model <- function(data, hyperparameters=get_default_hyperparameters(), verbose=FALSE){\n",
    "    \n",
    "    K <- attr(hyperparameters, \"single_model_K\")\n",
    "    CV_folds   <- get_CV_folds(nrow(data), K)\n",
    "    train_indices <- CV_folds[[1]]\n",
    "    test_indices <- CV_folds[[2]]\n",
    "    n_attempts <- attr(hyperparameters, \"n_attempts\")\n",
    "    max_power  <- attr(hyperparameters, \"max_power\")\n",
    "    starting_power <- attr(hyperparameters, \"starting_power\")\n",
    "    \n",
    "    response <- data$y\n",
    "    predictors <- data\n",
    "    predictors[[\"y\"]] <- NULL\n",
    "    n_predictors <- ncol(predictors)\n",
    "    \n",
    "    current_powers <- rep(starting_power, n_predictors)\n",
    "    best_powers <- rep(starting_power, n_predictors)\n",
    "    is_dummy <- rep(FALSE, n_predictors)\n",
    "    for(i in 1:length(is_dummy)){\n",
    "        is_dummy[i] <- length(unique(predictors[[i]])) <= 2\n",
    "    }\n",
    "    \n",
    "    best_score <- Inf\n",
    "    \n",
    "    null_model_score <- 0\n",
    "    \n",
    "    for(n in 1:n_attempts){\n",
    "        \n",
    "        if(n == 1){\n",
    "            #do nothing here in the first iteration\n",
    "        }\n",
    "        else{\n",
    "            #in iterations after the first, make random changes to the model\n",
    "            \n",
    "            \n",
    "            \n",
    "            index <- sample(1:n_predictors, 1)\n",
    "            sign  <- sample(c(-1, 1), 1)\n",
    "            if(current_powers[index] == max_power && sign==1){\n",
    "                sign <- -1\n",
    "            }\n",
    "            if(current_powers[index]==0 && sign==-1){\n",
    "                sign <-  1\n",
    "            }\n",
    "            if(is_dummy[index] && current_powers[index] == 1){\n",
    "                sign <- -1\n",
    "            }\n",
    "            \n",
    "            current_powers[index] <- current_powers[index] + sign\n",
    "            \n",
    "            \n",
    "            \n",
    "        }\n",
    "        if(verbose){\n",
    "            print(paste(\"Evaluating model:\", paste(current_powers, sep=\" \", collapse=\", \"), sep=\" \"))\n",
    "        }\n",
    "        \n",
    "        oof_scores <- rep(0, K)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for(k in 1:K){\n",
    "            if(sum(current_powers)==0){\n",
    "                #empty model - just use mean as intercept\n",
    "                train_Y <- response[train_indices[[k]]]\n",
    "                test_Y  <- response[test_indices[[k]]] \n",
    "                prediction <- mean(train_Y)\n",
    "                oof_scores[k] <- mean(abs(test_Y - prediction))\n",
    "            }\n",
    "            else{\n",
    "                \n",
    "                \n",
    "                    \n",
    "                y_pred <- polyreg(data, current_powers, train_indices[[k]], test_indices[[k]])\n",
    "            \n",
    "                oof_scores[k] <- mean(abs(response[test_indices[[k]]] - y_pred))\n",
    "            }\n",
    "\n",
    "        }#end loop over folds\n",
    "        \n",
    "        if(n==1){\n",
    "            starting_model_score <- mean(oof_scores)\n",
    "        }\n",
    "        \n",
    "        #if the model is an improvement over previous best, update it\n",
    "        final_score <- mean(oof_scores)\n",
    "        \n",
    "        if(final_score < best_score){\n",
    "            output_message = \"Keeping model\"\n",
    "            best_score <- final_score\n",
    "            best_powers <- current_powers\n",
    "        }\n",
    "        else{\n",
    "            output_message = \"Discarding model\"\n",
    "            current_powers <- best_powers\n",
    "        }\n",
    "        if(verbose){\n",
    "            print(paste(\"Model score =\", final_score, sep=\" \"))\n",
    "            print(output_message)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #return description of this model\n",
    "    #as a list of predictor names (strings)\n",
    "    return(list(best_score/starting_model_score, best_powers))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if data files are present in local directory\n",
    "#if they are not, try to download\n",
    "#return 1 if files are now present locally (either found or successfully downloaded)\n",
    "#return 0 otherwise\n",
    "#if this returns 1, we expect a subsequent call to read_data to succeed\n",
    "find_or_download_data <- function (main_url, files){\n",
    "\n",
    "    for (i in 1:length(files)){\n",
    "        if(!file.exists(files[i])){\n",
    "            print(paste(\"Could not find\", files[i], \"- attempting to download\", sep=\" \" ))\n",
    "            download.file(paste(main_url, files[i],sep=\"\") , files[i], \"auto\", quiet = FALSE)\n",
    "        }\n",
    "    }\n",
    "    for (i in 1:length(files)){\n",
    "        if(!file.exists(files[i])){\n",
    "            print(paste(\"Could not find or download\", files[i], \"- training will fail\", sep=\" \" ))\n",
    "            return(0)\n",
    "        }\n",
    "    }\n",
    "    return(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from .csv files to data.frame\n",
    "#merge multiple .csv into one (binds rows)\n",
    "#add a one-hot encoding of the file number\n",
    "read_data <- function(files){\n",
    "    single_datasets <- vector(\"list\", length(files))\n",
    "    for (i in 1:length(files)){\n",
    "        single_datasets[[i]] <- read.csv(files[i], header=TRUE, sep=\";\")\n",
    "        single_datasets[[i]]$file.number <- i\n",
    "    }\n",
    "    data <- bind_rows(single_datasets) \n",
    "    #bind_rows : dplyr function\n",
    "    \n",
    "    #data is now full dataset (merged across all files)\n",
    "        \n",
    "    #make one-hot encoding of file number\n",
    "    u <- unique(data$file.number)\n",
    "    for (val in u[1:(length(u)-1)]){\n",
    "        data[paste(\"file.number.\",val,sep=\"\")] <- (data$file.number==val)*1.0\n",
    "    }\n",
    "    data$file.number <- NULL\n",
    "    #remove initial (categorical) file number column\n",
    "    return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_CV_folds <- function(N, K){\n",
    "    train_indices <- vector(\"list\", K)\n",
    "    test_indices  <- vector(\"list\", K)\n",
    "    random_permutation <- sample(N, N, replace=FALSE)\n",
    "    for (i in 1:K){\n",
    "        start <- floor(N*(i-1)/K) + 1\n",
    "        stop  <- floor(N*i/K)\n",
    "        test_indices[[i]] <-  random_permutation[start:stop]\n",
    "        train_indices[[i]] <- random_permutation[-(start:stop)]\n",
    "    }\n",
    "    return(list(train_indices, test_indices))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define shorter names for this specific dataset\n",
    "#to help plotting\n",
    "short_names <- function(){\n",
    "    sn <- list(fixed.acidity=\"fix.acd\",\n",
    "               volatile.acidity=\"vol.acd\",\n",
    "                citric.acid=\"cit.acd\",\n",
    "                residual.sugar=\"res.sug\",\n",
    "                chlorides=\"chlor\",\n",
    "                free.sulfur.dioxide=\"fr.SO2\",\n",
    "                total.sulfur.dioxide=\"tot.SO2\",\n",
    "                density=\"dens\",\n",
    "                pH=\"pH\",\n",
    "                sulphates=\"sulph\",\n",
    "                alcohol=\"alc\",\n",
    "                file.number.1=\"red\")\n",
    "    return(sn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function for evaluating different approaches\n",
    "evaluate_ensemble <- function(hyperparameters){\n",
    "\n",
    "    verbose <- attr(hyperparameters, \"verbose\")\n",
    "    K <- attr(hyperparameters, \"ensemble_K\")\n",
    "    R <- attr(hyperparameters, \"top_level_repeats\")\n",
    "    \n",
    "    main_url <- attr(hyperparameters, \"main_url\")\n",
    "    files    <- attr(hyperparameters, \"files\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    have_data <- find_or_download_data(main_url, files)\n",
    "    if(!have_data){\n",
    "        print(\"Could not find or download data - returning null\")\n",
    "        return(NULL)\n",
    "    }\n",
    "    else{\n",
    "        \n",
    "        data <- read_data(files)\n",
    "        \n",
    "        data <- preprocess(data, response_name)\n",
    "        \n",
    "        CV_indices <- get_CV_folds(nrow(data), K)\n",
    "        train_indices <- CV_indices[[1]]\n",
    "        test_indices  <- CV_indices[[2]]\n",
    "        \n",
    "        score_matrix = matrix(0, R, K)\n",
    "        \n",
    "        for (r in 1:R){\n",
    "            if(verbose){\n",
    "                #print(paste('Top-level repeat', r, sep=' '))\n",
    "            }\n",
    "            for (k in 1:K){\n",
    "                if(verbose){\n",
    "                    #print(paste('Top-level CV fold', k, sep=' '))\n",
    "                }\n",
    "                if(attr(hyperparameters, \"run_standard_model\")){\n",
    "                    #just fit one linear model to original predictors\n",
    "                    #for comparison\n",
    "                    train_data <- data[train_indices[[k]],]\n",
    "                    test_data  <- data[test_indices[[k]],]\n",
    "                    model  <- lm(y ~ (.)^2, data=train_data)\n",
    "                    score_matrix[r,k]  <- mean(abs(test_data$y- predict(model, newdata=test_data)))\n",
    "                }\n",
    "                else{\n",
    "                    #first step: find an ensemble of models\n",
    "                    #by searching the space of possible models\n",
    "                    #(this is the space of 2^P possible subsets of all predictors)\n",
    "                    outputs <- find_ensemble(data[train_indices[[k]],])\n",
    "                    scores <- outputs[[1]]\n",
    "                    ensemble_definitions <- outputs[[2]]\n",
    "                    if(verbose){\n",
    "                        print('Model definitions:')\n",
    "                        print(ensemble_definitions)\n",
    "                        print('Scores:')\n",
    "                        print(scores)\n",
    "                    }\n",
    "                    #now train the ensemble on the same data\n",
    "                    ensemble_models <- train_ensemble(ensemble_definitions, data[train_indices[[k]],])\n",
    "                    \n",
    "                    predictions <- run_ensemble(ensemble_models, data, train_indices[[k]], test_indices[[k]], scores, hyperparameters)\n",
    "                    score_matrix[r,k] <- mean(abs(predictions - data$y[test_indices[[k]]]))\n",
    "                    #print(paste(\"Ensemble score =\",score,sep=\" \"))\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    dim(scores) <- NULL\n",
    "    return(scores)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model, newdata = full_data[test_indices, ]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model, newdata = full_data[test_indices, ]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model, newdata = full_data[test_indices, ]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model, newdata = full_data[test_indices, ]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model, newdata = full_data[test_indices, ]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"until reg\"\n",
      "[1] \"Model definitions:\"\n",
      "[[1]]\n",
      " [1] 1 1 1 1 0 2 2 1 1 1 1 1\n",
      "\n",
      "[[2]]\n",
      " [1] 1 1 1 0 0 2 2 1 1 1 1 1\n",
      "\n",
      "[[3]]\n",
      " [1] 1 2 1 1 1 1 1 1 1 2 1 1\n",
      "\n",
      "[1] \"Scores:\"\n",
      "[1] 0.9901498 0.9909394 0.9879349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“the response appeared on the right-hand side and was dropped”Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“problem with term 1 in model.matrix: no columns are assigned”Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“the response appeared on the right-hand side and was dropped”Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“problem with term 1 in model.matrix: no columns are assigned”Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“the response appeared on the right-hand side and was dropped”Warning message in model.matrix.default(mt, mf, contrasts):\n",
      "“problem with term 1 in model.matrix: no columns are assigned”Warning message in if (powers[[p]] > 0) {:\n",
      "“the condition has length > 1 and only the first element will be used”Warning message in if (powers[[p]] > 0) {:\n",
      "“the condition has length > 1 and only the first element will be used”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in polym(x, ..., degree = degree, coefs = coefs, raw = raw): arguments must have the same length\n",
     "output_type": "error",
     "traceback": [
      "Error in polym(x, ..., degree = degree, coefs = coefs, raw = raw): arguments must have the same length\nTraceback:\n",
      "1. evaluate_ensemble(hp)",
      "2. run_ensemble(ensemble_models, data, train_indices[[k]], test_indices[[k]], \n .     scores, hyperparameters)   # at line 61 of file <text>",
      "3. polyreg(data, powers, train_indices, test_indices)   # at line 15 of file <text>",
      "4. poly(predictors[[p]], powers[[p]])   # at line 10 of file <text>",
      "5. polym(x, ..., degree = degree, coefs = coefs, raw = raw)",
      "6. stop(\"arguments must have the same length\")"
     ]
    }
   ],
   "source": [
    "#top level code: define hyperparameters, run ensemble\n",
    "hp <- get_default_hyperparameters()\n",
    "\n",
    "attr(hp, \"verbose\") <- TRUE\n",
    "\n",
    "attr(hp, \"run_standard_model\") <- TRUE\n",
    "scores_1 <- evaluate_ensemble(hp)\n",
    "attr(hp, \"run_standard_model\") <- FALSE\n",
    "scores_2 <- evaluate_ensemble(hp)\n",
    "\n",
    "print(paste(\"mean of scores_1 = \", mean(scores_1), sep=\" \"))\n",
    "print(paste(\"mean of scores_2 = \", mean(scores_2), sep=\" \"))\n",
    "print(paste(\"SEM of scores_1 = \", sd(scores_1)/sqrt(length(scores_1)), sep=\" \"))\n",
    "print(paste(\"SEM of scores_2 = \", sd(scores_2)/sqrt(length(scores_2)), sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function for EDA\n",
    "library(gridExtra)\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "\n",
    "\n",
    "reorder_cormat <- function(cormat){\n",
    "    # Use correlation between variables as distance\n",
    "    dd <- as.dist((1-cormat)/2)\n",
    "    hc <- hclust(dd)\n",
    "    cormat <-cormat[hc$order, hc$order]\n",
    "}\n",
    "\n",
    "get_breaks <- function(x, margin, n_breaks){\n",
    "    m <- margin*(max(x) - min(x))\n",
    "    \n",
    "    return(seq(min(x) + m, max(x) - m, (max(x) - min(x) - 2*m)/(n_breaks-1)))\n",
    "}\n",
    "\n",
    "run_eda <- function(to_show, hyperparameters, corr_cutoff=0.5, boxplot_predictor_groups=3){\n",
    "    main_url <- attr(hyperparameters, \"main_url\")\n",
    "    files    <- attr(hyperparameters, \"files\")\n",
    "    response_name <- attr(hyperparameters, \"response_name\")\n",
    "    \n",
    "    have_data <- find_or_download_data(main_url, files)\n",
    "    if(!have_data){\n",
    "        print(\"Could not find or download data - returning null\")\n",
    "        return(NULL)\n",
    "    }\n",
    "    else{\n",
    "        data <- read_data(files)\n",
    "        \n",
    "        #plot relationship between each predictor and response\n",
    "        predictor_names <- colnames(data)[colnames(data) != response_name]\n",
    "        \n",
    "        if(to_show==\"pairs\"){\n",
    "        #pairs plot\n",
    "            \n",
    "            sn <- short_names()\n",
    "            labels <- list()\n",
    "            for (i in 1:length(predictor_names)){\n",
    "                labels[i] <- sn[[predictor_names[i]]]\n",
    "            }\n",
    "        \n",
    "            pairs(data[,predictor_names], labels=labels, panel=points, pch = 16, cex = .5, xaxt = \"n\", yaxt = \"n\")\n",
    "            \n",
    "            #alternative: ggally pairs plot\n",
    "            #this does not handle the large number of variables well\n",
    "        \n",
    "            ## code for ggally pairs plot:\n",
    "            #pairplot_columns <- 1:length(colnames(data))\n",
    "            #pairplot_columns <- pairplot_columns[colnames(data) != response_name]\n",
    "            #pairplot <- ggpairs(data, columns=pairplot_columns) + \n",
    "            #  ggtitle(\"Pairs plot\")\n",
    "            #print(pairplot)\n",
    "        }\n",
    "        else if(to_show==\"corr_heatmap\"){\n",
    "        #correlation heatmap\n",
    "        \n",
    "            predictors <- data[,predictor_names]\n",
    "            colnames(predictors) <- labels\n",
    "        \n",
    "            cormat <- round(cor(predictors),2)\n",
    "            cormat <- reorder_cormat(cormat)\n",
    "        \n",
    "            cormat[lower.tri(cormat)]<- NA\n",
    "        \n",
    "            # Melt the correlation matrix\n",
    "            melted_cormat <- melt(cormat, na.rm = TRUE)\n",
    "            # Heatmap\n",
    "            ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+\n",
    "                 geom_tile(color = \"white\")+\n",
    "                 scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n",
    "                 midpoint = 0, limit = c(-1,1), space = \"Lab\", \n",
    "                 name=\"Pearson\\nCorrelation\") +\n",
    "                 theme_minimal()+ \n",
    "                 theme(axis.text.x = element_text(angle = 45, vjust = 1, \n",
    "                 size = 12, hjust = 1))+\n",
    "                 coord_fixed()\n",
    "        }\n",
    "        else if(to_show==\"corr_values\"){\n",
    "        #print pairs of variables with largest absolute correlation values\n",
    "\n",
    "            melted_cormat <- melt(cormat, na.rm=TRUE)\n",
    "            melted_cormat <- melted_cormat[order(abs(melted_cormat$value)),]\n",
    "            print(melted_cormat[abs(melted_cormat$value) > corr_cutoff & melted_cormat$Var1 != melted_cormat$Var2,])\n",
    "        }\n",
    "        else if(to_show==\"response_hist\"){\n",
    "        #plot histogram of response values\n",
    "            par(pin=c(4,3))\n",
    "            vals <- data[[response_name]]\n",
    "            h <- hist(vals, xlab=response_name, ylab=\"count\", breaks=seq(min(vals)-0.5, max(vals) + 0.5, 1), \n",
    "                 main=paste(\"Histogram of \", response_name, sep=\" \"),ylim=c(0, 3000))\n",
    "            #plot(h, )\n",
    "            scale <- length(vals)\n",
    "            curve(dnorm(x, mean=mean(vals), sd=sd(vals))*scale, add=TRUE, col=\"darkblue\", lwd=2)\n",
    "\n",
    "        }\n",
    "        else if(to_show==\"boxplots_grouped_by_response\"){\n",
    "        #plot series of boxplots grouped by values of response (on x axis)\n",
    "        #only works if response takes discrete values\n",
    "        #has some limitations, but can be useful\n",
    "            \n",
    "            label_format <- function(x) sprintf(\"%.2f\", x)\n",
    "            breaks <- function(x) get_breaks(x, 0.05, 4)\n",
    "            \n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    plots[[i]] <- ggplot(data, aes_string(group=response_name, y = x, x = response_name)) + \n",
    "                        geom_boxplot(outlier.size=0.15) + geom_smooth(aes(group=1), method=\"lm\") + \n",
    "                        scale_y_continuous(labels=label_format) + \n",
    "                        theme(axis.text.x = element_text(color = \"grey20\", size = 8, angle = 90, hjust = .5, vjust = .5, face = \"plain\")) +\n",
    "                        coord_flip()\n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        }\n",
    "        else if(to_show==\"boxplots_predictor_on_x\"){\n",
    "        #plot series of boxplots grouped by values of predictor (on x axis)  \n",
    "        #divide predictor into quantiles for grouping\n",
    "        #this tends to give ugly plots with issues that make them hard to interpret\n",
    "        #not used further\n",
    "            qstep <- 1/boxplot_predictor_groups\n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    quantile_boundaries <- quantile(data[[x]], probs=seq(0,1,qstep))\n",
    "                    quantile_groups <- rep(0, length(data[[x]]))\n",
    "                    for (q in quantile_boundaries){\n",
    "                        quantile_groups <- quantile_groups + 1.0*(data[[x]] > q)\n",
    "                    }\n",
    "                    plots[[i]] <- ggplot(data, aes_string(group=quantile_groups, y = response_name, x = x)) + \n",
    "                    geom_boxplot(outlier.size=0.15) + geom_smooth(aes(group=1), method=\"lm\") \n",
    "                        \n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        }\n",
    "        else if(to_show==\"separate_regressions\"){\n",
    "            \n",
    "            label_format <- function(x) sprintf(\"%.2f\", x)\n",
    "            \n",
    "            nplot <- length(predictor_names)\n",
    "            nrow <- floor(sqrt(nplot))\n",
    "            ncol <- ceiling(nplot/nrow)\n",
    "            plots <- list()\n",
    "            for(i in 1:length(predictor_names)){\n",
    "                x <- predictor_names[i]\n",
    "                if(length(unique(data[[x]])) > 2){\n",
    "                    #exclude any one-hot encodings\n",
    "                    plots[[i]] <- ggplot(data, aes_string(y = x, x = response_name)) + \n",
    "                        geom_count() + geom_smooth(aes(group=1), method=\"lm\") + theme(legend.position = \"none\") +\n",
    "                        scale_y_continuous(labels=label_format) + \n",
    "                        theme(axis.text.x = element_text(color = \"grey20\", size = 8, angle = 90, hjust = .5, vjust = .5, face = \"plain\")) +\n",
    "                        coord_flip()\n",
    "                    #have to add aes(group=1) to make smoothing work\n",
    "                }\n",
    "            }\n",
    "            grid.arrange(grobs=plots, ncol=ncol)\n",
    "        } \n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
